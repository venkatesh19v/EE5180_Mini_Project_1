# -*- coding: utf-8 -*-
"""Log_reg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zqDVpwz0_FX1QN-xXgRu8XPyWJgC6izV
"""

# Commented out IPython magic to ensure Python compatibility.
# Some libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, classification_report, precision_recall_curve
# %matplotlib inline

# Set default plot size
plt.rcParams["figure.figsize"] = (10, 6)

# Load the data
data = pd.read_csv('song_data.csv')

# View first few rows
data.head()

data

print(data.isnull().sum())

# Create the target variable 'danceable'
data['danceable'] = data['danceability'].apply(lambda x: 1 if x >= 0.5 else 0)
data.head()

# Define X (features) and y (target)
# We'll exclude non-numeric columns like 'song_name', and also 'danceability' itself as it is the target.
X = data.drop(columns=['song_name', 'danceability', 'danceable'])  # Drop non-numeric and target-related columns
y = data['danceable']

# Checking if any null values
# print(data)
print(data.isnull().sum())

data1 = data.drop('song_name', axis = 1)

plt.figure(figsize = (14,10))
sns.heatmap(data1.corr(), fmt = '.0%', annot = True, cmap = 'Blues')
plt.title('Correlation Heatmap')

data.hist(bins=30, figsize=(15, 10))
plt.suptitle('Distribution of Numeric Variables')
plt.show()

data1 = data.drop(['key','liveness','audio_mode'], axis = 1)
data1.head()

# Spliting data into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(X_train.shape) #rxc
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

print("DATA: ",data1.shape) #rxc
print("X_train: ",X_train) #rxc
print("X_test: ",X_test)
print("y_train: ",y_train)
print("y_test: ",y_test)

# data['target'] = (data['danceability'] >= 0.5).astype(int)

# # Use 'target' instead of 'danceable' for filtering
# target_dict = {
#     "Not Danceable": data[data['target'] == 0]['danceability'],
#     "Danceable": data[data['target'] == 1]['danceability']
# }

# plt.rcParams["font.size"] = 14

# fig, ax = plt.subplots(1, 2, figsize = (20,6))
# # Distribution of Danceability
# ax[1].boxplot(target_dict.values(), labels = target_dict.keys())
# ax[1].set_title("Distribution of Danceability ('Not Danceable' vs 'Danceable')")
# ax[1].set_ylabel("Danceability")

# plt.tight_layout()
# plt.show()

# Check for danceability values > 0.5 in the "Not Danceable" group
# print(data[(data['target'] == 0) & (data['danceability'] >= 0.5)])

data['target'] = (data['danceability'] < 0.5).astype(int)
print(data['target'].value_counts())

# Create the target column
# df['danceable'] = df['danceability'].apply(lambda x: 1 if x >= 0.5 else 0)

data['target'] = (data['danceability'] >= 0.5).astype(int)

target_dict = {
    "Not Danceable": data[data['danceable'] == 0]['danceability'],
    "Danceable": data[data['danceable'] == 1]['danceability']
}

plt.rcParams["font.size"] = 14

fig, ax = plt.subplots(1, 2, figsize = (20,6))

# Scatter plot: Energy vs Danceability
sns.scatterplot(x = "danceability", y = "energy", hue = "target",
                palette = ["b", "r"], data = data, ax = ax[0])
ax[0].set_title("Energy of the Song vs. Danceability")
ax[0].set_xlabel("Danceability")
ax[0].set_ylabel("Energy")

# Distribution of Danceability
ax[1].boxplot(target_dict.values(), labels = target_dict.keys())
ax[1].set_title("Distribution of Danceability ('Not Danceable' vs 'Danceable')")
ax[1].set_ylabel("Danceability")

plt.tight_layout()
plt.show()

print(X_train, y_train.shape)

# Split data into training and testing sets (without scaling)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model
log_reg_model = LogisticRegression(max_iter=1000)

# Train the model
log_reg_model.fit(X_train, y_train)

# Predicting on the test set
y_pred = log_reg_model.predict(X_test)

# Evaluate the model accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of Logistic Regression model: {accuracy * 100:.2f}%")

# Create the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix using Seaborn
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Danceable', 'Danceable'], yticklabels=['Not Danceable', 'Danceable'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Predict probabilities for the ROC curve
y_probs = log_reg_model.predict_proba(X_test)[:,1]

# Compute ROC curve and area under the curve (AUC)
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title('ROC Curve (No Scaling)')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

# Get classification report as a dictionary
report = classification_report(y_test, y_pred, output_dict=True)

# Convert the dictionary to a DataFrame for easier plotting
report_df = pd.DataFrame(report).transpose()

# Create a heatmap for the classification report
plt.figure(figsize=(10, 6))
sns.heatmap(report_df.iloc[:-1, :-1], annot=True, cmap="Blues", fmt=".2f")
plt.title('Classification Report')
plt.show()
print(classification_report(y_test, y_pred))

# Apply Standard Scaling to the features
std_scaler = StandardScaler()
X_std_scaled = std_scaler.fit_transform(X)

# Split the data into training and testing sets
X_train_std, X_test_std, y_train_std, y_test_std = train_test_split(X_std_scaled, y, test_size=0.2, random_state=42)

# Initialize and train logistic regression model
log_reg_std = LogisticRegression(max_iter=1000)
log_reg_std.fit(X_train_std, y_train_std)

# Predict on the test set
y_pred_std = log_reg_std.predict(X_test_std)

# Evaluate model performance
accuracy_std = accuracy_score(y_test_std, y_pred_std)
print(f"Accuracy with Standard Scaling: {accuracy_std * 100:.2f}%")

# Classification report for Standard scaled data
print("Classification Report (Standard Scaling):")
print(classification_report(y_test_std, y_pred_std))

# Get classification report as a dictionary
report = classification_report(y_test, y_pred, output_dict=True)

# Convert the dictionary to a DataFrame for easier plotting
report_df = pd.DataFrame(report).transpose()

# Create a heatmap for the classification report
plt.figure(figsize=(10, 6))
sns.heatmap(report_df.iloc[:-1, :-1], annot=True, cmap="Blues", fmt=".2f")
plt.title('Classification Report')
plt.show()

y_probs_std = log_reg_std.predict_proba(X_test_std)[:, 1]
fpr_std, tpr_std, thresholds_std = roc_curve(y_test_std, y_probs_std)
roc_auc_std = auc(fpr_std, tpr_std)

plt.plot(fpr_std, tpr_std, label=f'ROC Curve (AUC = {roc_auc_std:.2f})', color='green')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title('ROC Curve (Standard Scaling)')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

# Compute precision-recall curve
precision, recall, thresholds = precision_recall_curve(y_test, y_probs)

# Plot the precision-recall curve
plt.figure(figsize=(8,6))
plt.plot(recall, precision, color='green', lw=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

# Apply Min-Max Scaling to the features
min_max_scaler = MinMaxScaler()
X_mm_scaled = min_max_scaler.fit_transform(X)

X_train_mm, X_test_mm, y_train_mm, y_test_mm = train_test_split(
    X_mm_scaled, y, test_size=0.2, random_state=42)

# Initialize and train logistic regression model
log_reg_mm = LogisticRegression(max_iter=1000)
log_reg_mm.fit(X_train_mm, y_train_mm)

# Predict on the test set
y_pred_mm = log_reg_mm.predict(X_test_mm)

# Evaluate model performance
accuracy_mm = accuracy_score(y_test_mm, y_pred_mm)
print(f"Accuracy with Min-Max Scaling: {accuracy_mm * 100:.2f}%")

# Classification report
print("Classification Report (Min-Max Scaling):")
print(classification_report(y_test_mm, y_pred_mm))

# Confusion Matrix
cm_mm = confusion_matrix(y_test_mm, y_pred_mm)
sns.heatmap(cm_mm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Danceable', 'Danceable'],
            yticklabels=['Not Danceable', 'Danceable'])
plt.title('Confusion Matrix (Min-Max Scaling)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
y_probs_mm = log_reg_mm.predict_proba(X_test_mm)[:, 1]
fpr_mm, tpr_mm, thresholds_mm = roc_curve(y_test_mm, y_probs_mm)
roc_auc_mm = auc(fpr_mm, tpr_mm)

plt.plot(fpr_mm, tpr_mm, label=f'ROC Curve (AUC = {roc_auc_mm:.2f})', color='red')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title('ROC Curve (Min-Max Scaling)')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

# Compare accuracies
accuracies = {
    'No Scaling': accuracy,
    'Standard Scaling': accuracy_std,
    'Min-Max Scaling': accuracy_mm
}

plt.bar(accuracies.keys(), [v * 100 for v in accuracies.values()], color=['blue', 'green', 'red'])
plt.ylabel('Accuracy (%)')
plt.title('Comparison of Accuracies')
plt.tight_layout()
plt.show()

# Print summary
print("Accuracy Comparison:")
for key, value in accuracies.items():
    print(f"{key}: {value * 100:.2f}%")

