# -*- coding: utf-8 -*-
"""MiniProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bLyKbF0-CWnZo7RkMg7T0gOr1d5S5MUi

## Instructions


If you find yourself unsure about something; then please make an appropriate assumption and explain/document your rationale.

## Linear Regression

Please download the song_data.csv for Linear Regression and Logistic Regression assignments.

The audio features include attributes about the music track itself, such as duration, key, year. The metadata uses more abstract features, such as danceability, energy, or song hotttnesss.

---


Train a regression model (of your choice) to identify the energy of the song.
1.   Briefly explain why you have chosen this model.  
2.   Note down your cost.
"""

# Some libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression,LogisticRegression
from sklearn.metrics import accuracy_score
import numpy as np

#Load data


# Data Pre-Processing

# Drop non-numeric datapoints

# Define your X  and y

#Divide the data into testing dataset and training dataset

#Training

# Testing

"""## Logistic Regression

Using the same data as above and train the model to classify it as whether it is danceable or not.

We define a song as danceable when the dancability score of the song is $\geq0.5$ and not not dancable when the dancablility score $<0.5$.
"""

# Define your X and y


#Divide the data into testing dataset and training dataset


#Training


# Testing

"""## Perceptron

Q. Consider the following code to generate two datasets: one linear (X1, y1) and another non-linear (X2, y2). You can change parameters for your convenience.

from sklearn import datasets
X1, y1 = datasets.make_classification(n_features=2, n_classes=2, n_samples=200, n_redundant=0, n_clusters_per_class=1)
X2, y2 = datasets.make_circles(n_samples=200, noise=0.03, factor=0.7)

(a) Implement the Perceptron algorithm from scratch. Verify if the algorithm takes atmost (RB)^2 iterations. R and B has usual meanings (Refer Theorem 9.1 in UML book)
    [Use (X1, y1) dataset]

(b) Now use the (X2, y2) dataset and give an idea to solve the problem as a Binary classification problem using Perceptron.
    [Web Resource: https://towardsdatascience.com/perceptron-explanation-implementation-and-a-visual-example-3c8e76b4e2d1]
"""

import numpy as np
from sklearn import datasets

# Generate a linearly separable dataset
X1, y1 = datasets.make_classification(n_features=2, n_classes=2, n_samples=200,
                                      n_redundant=0, n_clusters_per_class=1)
y1 = np.where(y1 == 0, -1, 1)  # Convert labels to -1, +1

# Initialize variables
w = np.zeros(X1.shape[1])
b = 0
learning_rate = 1

# Calculate R (maximum norm of input samples)
R = np.max(np.linalg.norm(X1, axis=1))

# B is assumed to be the norm of some optimal hyperplane weight vector (we estimate this)
# For this example, let's assume B = 1 (as it's unknown initially)
B = 1
max_iterations = int((R * B) ** 2)

# Training loop
for t in range(max_iterations):
    error_count = 0
    for xi, yi in zip(X1, y1):
        if yi * (np.dot(w, xi) + b) <= 0:
            # Update weights and bias
            w += learning_rate * yi * xi
            b += learning_rate * yi
            error_count += 1
    if error_count == 0:
        print(f"Converged in {t} iterations")
        break
else:
    print(f"Did not converge within {max_iterations} iterations")

# Verify the theoretical bound
print(f"Number of iterations: {t}")
print(f"Theoretical upper bound: {(R * B) ** 2}")



"""Q. Implement  3Ã—1 MUX  for a boolean function with three variables (Use Pytorch)
                    (i) Make simple AND, OR, NOT logic gates using one layer perceptron combine them to make XOR. (XOR: F(A,B,C) = A'B + AB'+ B'C + BC').
                    (ii) Make a MLP for XOR gate of 3 variables and plot the decision boundary.
                    (iii) Comment on the complexity of both.

Useful Web Resources: https://d1b10bmlvqabco.cloudfront.net/attach/jl2b00mpen3au/jl2b0jnvzgn3hf/jn9f4p17wi31/CMSC422Perceptrons.pdf]"""

# part 1
import torch
import torch.nn as nn
import torch.optim as optim

# Define the Perceptron class for a simple logic gate
class Perceptron(nn.Module):
    def __init__(self):
        super(Perceptron, self).__init__()
        self.linear = nn.Linear(2, 1)

    def forward(self, x):
        return torch.sigmoid(self.linear(x))

# AND gate
and_gate = Perceptron()
and_gate.linear.weight = nn.Parameter(torch.tensor([[1.0, 1.0]]))
and_gate.linear.bias = nn.Parameter(torch.tensor([-1.5]))

# OR gate
or_gate = Perceptron()
or_gate.linear.weight = nn.Parameter(torch.tensor([[1.0, 1.0]]))
or_gate.linear.bias = nn.Parameter(torch.tensor([-0.5]))

# NOT gate
def not_gate(x):
    return 1 - x

# Define the XOR function using the AND, OR, and NOT gates
def xor_gate(A, B, C):
    A_not = not_gate(A)
    B_not = not_gate(B)
    C_not = not_gate(C)

    term1 = and_gate(torch.tensor([A_not, B]))  # A'B
    term2 = and_gate(torch.tensor([A, B_not])) # AB'
    term3 = and_gate(torch.tensor([B_not, C])) # B'C
    term4 = and_gate(torch.tensor([B, C_not])) # BC'

    return or_gate(torch.tensor([term1.item(), term2.item(), term3.item(), term4.item()]))

class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.hidden = nn.Linear(3, 4)  # Hidden layer with 4 neurons
        self.output = nn.Linear(4, 1)  # Output layer

    def forward(self, x):
        x = torch.sigmoid(self.hidden(x))
        x = torch.sigmoid(self.output(x))
        return x

# Training data for XOR with 3 variables
X_train = torch.tensor([
    [0, 0, 0],
    [0, 0, 1],
    [0, 1, 0],
    [0, 1, 1],
    [1, 0, 0],
    [1, 0, 1],
    [1, 1, 0],
    [1, 1, 1]
], dtype=torch.float32)

y_train = torch.tensor([0, 1, 1, 0, 1, 0, 0, 1], dtype=torch.float32).view(-1, 1)

# Initialize model, loss function, and optimizer
model = MLP()
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

# Training loop
num_epochs = 10000
for epoch in range(num_epochs):
    optimizer.zero_grad()
    output = model(X_train)
    loss = criterion(output, y_train)
    loss.backward()
    optimizer.step()
    if epoch % 1000 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item()}')

import numpy as np
import matplotlib.pyplot as plt

# Generate a grid of points
x1 = np.linspace(0, 1, 100)
x2 = np.linspace(0, 1, 100)
x3 = np.linspace(0, 1, 100)
xx, yy, zz = np.meshgrid(x1, x2, x3)
grid = torch.tensor(np.c_[xx.ravel(), yy.ravel(), zz.ravel()], dtype=torch.float32)

# Get predictions for each point in the grid
with torch.no_grad():
    predictions = model(grid).numpy().reshape(xx.shape)

# Plotting the decision boundary (visualize using one projection)
plt.contourf(xx[:, :, 0], yy[:, :, 0], predictions[:, :, 0], levels=[0, 0.5, 1], cmap="coolwarm", alpha=0.6)
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train.squeeze(), cmap='coolwarm', edgecolors='k')
plt.xlabel('X1')
plt.ylabel('X2')
plt.title('Decision Boundary for 3-Input XOR')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

class Perceptron:
    def __init__(self, learning_rate=1.0, max_iter=1000):
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        # Initialize weights and bias to zero
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for _ in range(self.max_iter):
            no_errors = True
            for idx, x_i in enumerate(X):
                linear_output = np.dot(x_i, self.weights) + self.bias
                y_predicted = np.sign(linear_output)
                if y[idx] * linear_output <= 0:
                    self.weights += self.learning_rate * y[idx] * x_i
                    self.bias += self.learning_rate * y[idx]
                    no_errors = False
            if no_errors:
                break

    def predict(self, X):
        linear_output = np.dot(X, self.weights) + self.bias
        return np.sign(linear_output)

# Generate a linearly separable dataset (X1, y1)
X1, y1 = datasets.make_classification(
    n_features=2, n_classes=2, n_samples=200, n_redundant=0, n_clusters_per_class=1
)
y1 = np.where(y1 == 0, -1, 1)  # Convert labels to -1 and 1

# Initialize and train the perceptron
perceptron = Perceptron(learning_rate=1.0, max_iter=1000)
perceptron.fit(X1, y1)

# Define the decision boundary function
def plot_decision_boundary(X, y, model):
    # Create a meshgrid
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

    # Calculate predictions for each point in the grid
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    Z = model.predict(grid_points)
    Z = Z.reshape(xx.shape)

    # Plot the decision boundary and data points
    plt.contourf(xx, yy, Z, levels=[-1, 0, 1], alpha=0.3, colors=['#FFAAAA', '#AAAAFF'])
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k', marker='o')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Perceptron Decision Boundary')
    plt.show()

# Plot the decision boundary with data points
plot_decision_boundary(X1, y1, perceptron)

from sklearn.preprocessing import PolynomialFeatures
from sklearn import datasets
import numpy as np

# Generate the non-linear dataset
X2, y2 = datasets.make_circles(n_samples=200, noise=0.03, factor=0.7)
y2 = np.where(y2 == 0, -1, 1)  # Convert labels to -1, +1

# Transform features to higher dimension (e.g., polynomial features)
poly = PolynomialFeatures(degree=2)
X2_transformed = poly.fit_transform(X2)

# Initialize variables
w = np.zeros(X2_transformed.shape[1])
b = 0
learning_rate = 1

# Calculate R (maximum norm of transformed input samples)
R = np.max(np.linalg.norm(X2_transformed, axis=1))

# B is assumed to be the norm of some optimal hyperplane weight vector (we estimate this)
B = 1
max_iterations = int((R * B) ** 2)

# Training loop
for t in range(max_iterations):
    error_count = 0
    for xi, yi in zip(X2_transformed, y2):
        if yi * (np.dot(w, xi) + b) <= 0:
            # Update weights and bias
            w += learning_rate * yi * xi
            b += learning_rate * yi
            error_count += 1
    if error_count == 0:
        print(f"Converged in {t} iterations")
        break
else:
    print(f"Did not converge within {max_iterations} iterations")

# Verify the theoretical bound
print(f"Number of iterations: {t}")
print(f"Theoretical upper bound: {(R * B) ** 2}")